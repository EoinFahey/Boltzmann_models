{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "fRh9GEiKPqVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "gc94xtMM1Bdn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive_path = '/content/gdrive/MyDrive/Boltzmann_machines'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7OIjKxDCOaT",
        "outputId": "1b02660c-c7a8-43e1-f116-335680cd0907"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sets\n",
        "# Paths\n",
        "movies_file = os.path.join(drive_path, 'ml-1m/movies.dat')\n",
        "users_file = os.path.join(drive_path, 'ml-1m/users.dat')\n",
        "ratings_file = os.path.join(drive_path, 'ml-1m/ratings.dat')\n",
        "\n",
        "#Load\n",
        "movies = pd.read_csv(movies_file, sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv(users_file, sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv(ratings_file, sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "\n",
        "# Display\n",
        "print(' MOVIES', movies.head(3), '\\n\\n', 'USERS', users.head(3), '\\n\\n', 'RATINGS', ratings.head(3))"
      ],
      "metadata": {
        "id": "7zKrBjDW8V-A",
        "outputId": "6a8e67b4-7397-4022-b603-cbdc604e3675",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " MOVIES    0                        1                             2\n",
            "0  1         Toy Story (1995)   Animation|Children's|Comedy\n",
            "1  2           Jumanji (1995)  Adventure|Children's|Fantasy\n",
            "2  3  Grumpier Old Men (1995)                Comedy|Romance \n",
            "\n",
            " USERS    0  1   2   3      4\n",
            "0  1  F   1  10  48067\n",
            "1  2  M  56  16  70072\n",
            "2  3  M  25  15  55117 \n",
            "\n",
            " RATINGS    0     1  2          3\n",
            "0  1  1193  5  978300760\n",
            "1  1   661  3  978302109\n",
            "2  1   914  3  978301968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training and test sets\n",
        "\n",
        "# Paths\n",
        "training_set_file = os.path.join(drive_path, 'ml-100k/u1.base')\n",
        "test_set_file = os.path.join(drive_path, 'ml-100k/u1.test')\n",
        "\n",
        "# Load\n",
        "# 0th is row, 1st column = user, 2nd = movie, 3rd = rating, 4th = timestamp\n",
        "training_set = pd.read_csv(training_set_file, delimiter = '\\t')\n",
        "test_set = pd.read_csv(test_set_file, delimiter = '\\t')\n",
        "\n",
        "# Display\n",
        "print(' TRAINING', training_set.head(3), '\\n\\n', 'TESTING', test_set.head(3))\n",
        "\n",
        "# Turn to arrays (same values still)\n",
        "training_set = np.array(training_set, dtype = 'int')\n",
        "test_set = np.array(test_set, dtype = 'int')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZBgMH7HBkeq",
        "outputId": "5af92215-42dc-42a3-aed9-f14a72e7a5de"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " TRAINING    1  1.1  5  874965758\n",
            "0  1    2  3  876893171\n",
            "1  1    3  4  878542960\n",
            "2  1    4  3  876893119 \n",
            "\n",
            " TESTING    1   6  5  887431973\n",
            "0  1  10  3  875693118\n",
            "1  1  12  5  878542960\n",
            "2  1  14  5  874965706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get total number of users and movies, across\n",
        "# Gives total across train and test data (cross-validation)\n",
        "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
        "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"
      ],
      "metadata": {
        "id": "ePkmK0J1HWFw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data into array with a user on each line and movies in columns\n",
        "def convert(data):\n",
        "    # Create list of list. Each list corresponds to a user, and their movie ratings\n",
        "    new_data = []\n",
        "    # Add ratings into user's list\n",
        "    for id_users in range(1, nb_users + 1):\n",
        "      id_movies = data[:, 1][data[:, 0] == id_users]\n",
        "      id_ratings = data[:, 2][data[:, 0] == id_users]\n",
        "      # Fill with zeros\n",
        "      ratings = np.zeros(nb_movies)\n",
        "      # Replace zeros with real ratings\n",
        "      ratings[id_movies - 1] = id_ratings\n",
        "      new_data.append(list(ratings))\n",
        "    return new_data\n",
        "\n",
        "# Contains 943 rows of lists. In each list is the user's ratings of each movie\n",
        "# Moves without a rating just have a 0\n",
        "training_set = convert(training_set)\n",
        "training_set = convert(test_set)"
      ],
      "metadata": {
        "id": "7yyCKXLYJW9P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data into Torch tensors to enable manipulation in PyTorch\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)"
      ],
      "metadata": {
        "id": "d5-oVnIkM1H_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up to here, all the data preprocessing could be used for other types of models. After here, it's specific to Botlzmann models"
      ],
      "metadata": {
        "id": "O5SrDBvdN6zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert ratings into binary (1 = liked, 0 = not liked, -1 = no rating)\n",
        "training_set[training_set == 0] = -1\n",
        "training_set[training_set == 1] = 0\n",
        "training_set[training_set == 2] = 0\n",
        "training_set[training_set >= 3] = 1\n",
        "\n",
        "test_set[test_set == 0] = -1\n",
        "test_set[test_set == 1] = 0\n",
        "test_set[test_set == 2] = 0\n",
        "test_set[test_set >= 3] = 1"
      ],
      "metadata": {
        "id": "m9adBwqGOGtH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boltzmann model"
      ],
      "metadata": {
        "id": "QuTwCNGCPwSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RBM():\n",
        "  # Self reference, visible nodes, hidden nodes\n",
        "  def __init__(self, nv, nh):\n",
        "    # Initialise weights\n",
        "    self.W = torch.randn(nh, nv)\n",
        "    # Probability of hidden nodes, given visible nodes. 1 = batch, nh = bias\n",
        "    self.a = torch.randn(1, nh)\n",
        "    # Bias for visible nodes\n",
        "    self.b = torch.randn(1, nv)\n",
        "  # Calculate probability that hidden neuron h = 1 given the visible neuron\n",
        "  def sample_h(self, x):\n",
        "    # Make product of two tensors. 'mm' does this for two torch tensors. x = visible neuron, w = tensor of weights\n",
        "    wx = torch.mm(x, self.W.t())\n",
        "    # Activation function. wx + bias. Apply bias to each line of mini-batch using expand_as\n",
        "    activation = wx + self.a.expand_as(wx)\n",
        "    # Probability that hidden node is activated, given visible node. Calculated as sigmoid of activation\n",
        "    p_h_given_v = torch.sigmoid(activation)\n",
        "    # If random number is below 70%, activate neuron, otherwise don't. Gives 0 & 1\n",
        "    return p_h_given_v, torch.bernoulli(p_h_given_v)\n",
        "  # Calculate probability that visible neuron v = 1 given the hidden neuron\n",
        "  def sample_v(self, y):\n",
        "    # Transpose not needed. W is weight matrix of pv given h so you need transpose for ph given v. Here it's just pv given h though so no transpose\n",
        "    wy = torch.mm(y, self.W())\n",
        "    # b, not a\n",
        "    activation = wy.self.b.expand_as(wy)\n",
        "    p_v_given_h = torch.sigmoid(activation)\n",
        "    # From vector of probabilities, give some sampling. If random number from sampling is below 0.25, give 1, otherwise 0\n",
        "    # Depending on 0 or 1, this is the prediction of whether or not user will give a like\n",
        "    return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
        "# v0 = input vector containing ratings of all the movies by one user\n",
        "# vk = visible nodes obtained after k-sampling\n",
        "# ph0 = vector of probabilities which at first iteration have hidden nodes h = 1, given the values of v0\n",
        "# phk = probability of hidden nodes after k-sampling given values of visible nodes vk\n",
        "# Other parameters like learning rate could be added to improve model\n",
        "def train(self, v0, vk, ph0, phk):\n",
        "  self.W += torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)\n",
        "  self.b += torch.sum((v0 - vk), 0)\n",
        "  self.a += torch.sum((ph0 - phk), 0)\n",
        "# Nodes are ratings of all movies by a user in this example so NV (number of visible nodes) is the number of movies\n",
        "nv = len(training_set[0])\n",
        "# nh can be any number but there are 1,682 movies. nh corresponds to actual features so maybe 100 features. Best to tune and try different numbers\n",
        "nh = 100\n",
        "# Also tuneable\n",
        "batch_size = 100\n",
        "# Create RBM object\n",
        "rbm = RBM(nv, nh)"
      ],
      "metadata": {
        "id": "4YWB9U9rPyh2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RBM is an energy-based model i.e., you're trying to minimize the energy function for a normal state. Energy depends on weighting of the model based on the tensors.\n",
        "\n",
        "Weights can be optimized to minimize the energy of the model. Minimizing the energy is equivalent to maximizing the log-likelihood gradient of the training set, so that's what you're computing in the model.\n",
        "\n",
        "Computing the log-likelihood gradient of the training set directly is too computationally expensive, so instead you can reach it through better and better approximations. Tiny adjustments in the direction of minimal energy.\n",
        "\n",
        "Contrastive divergence learning allows you to get those adjustments. You can do this using a Gibbs chain in k number of steps. K * hidden nodes and visible nodes. So given v0, sample the probable hidden nodes for v0 and then use those probable hidden nodes to sample the probable visible nodes. Repeat for v1, v2...vK. That's a CDK algorithm: k-step contrastive divergence."
      ],
      "metadata": {
        "id": "CY8l3_axe_CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training RBM model\n",
        "\n",
        "nb_epoch = 10\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "  # Need loss function to measure error between predictions and observations\n",
        "  # Could use RMSE (root mean squared error), but also simple distance or absolute distance\n",
        "  # Loss starts at 0 and increases with errors\n",
        "  train_loss = 0\n",
        "  # Also need a counter to normalize train_loss by dividing train_loss by counter\n",
        "  s = 0.\n",
        "  # Update weights in batches of users, not per each user. Batch size 100\n",
        "  for id_user in range(0, nb_users - batch_size, 100):\n",
        "    # Input is vector going into Gibbs chain. Ratings of all the users in the batch\n",
        "    vk = training_set[id_user : id_user + batch_size]\n",
        "    # Movie ratings from batch. Same at start, will be updated\n",
        "    v0 = training_set[id_user : id_user + batch_size]\n",
        "    ph0 =\n",
        "\n",
        "# (currently unfinished)\n"
      ],
      "metadata": {
        "id": "DzN87f1XmeqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}