{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "fRh9GEiKPqVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "gc94xtMM1Bdn"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive_path = '/content/gdrive/MyDrive/Boltzmann_machines'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7OIjKxDCOaT",
        "outputId": "d3623167-a486-4430-b1fa-a9c2db349a1c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sets\n",
        "# Paths\n",
        "movies_file = os.path.join(drive_path, 'ml-1m/movies.dat')\n",
        "users_file = os.path.join(drive_path, 'ml-1m/users.dat')\n",
        "ratings_file = os.path.join(drive_path, 'ml-1m/ratings.dat')\n",
        "\n",
        "#Load\n",
        "movies = pd.read_csv(movies_file, sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv(users_file, sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv(ratings_file, sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "\n",
        "# Display\n",
        "print(' MOVIES', movies.head(3), '\\n\\n', 'USERS', users.head(3), '\\n\\n', 'RATINGS', ratings.head(3))"
      ],
      "metadata": {
        "id": "7zKrBjDW8V-A",
        "outputId": "136246bb-9964-46af-e882-7766e592ed95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " MOVIES    0                        1                             2\n",
            "0  1         Toy Story (1995)   Animation|Children's|Comedy\n",
            "1  2           Jumanji (1995)  Adventure|Children's|Fantasy\n",
            "2  3  Grumpier Old Men (1995)                Comedy|Romance \n",
            "\n",
            " USERS    0  1   2   3      4\n",
            "0  1  F   1  10  48067\n",
            "1  2  M  56  16  70072\n",
            "2  3  M  25  15  55117 \n",
            "\n",
            " RATINGS    0     1  2          3\n",
            "0  1  1193  5  978300760\n",
            "1  1   661  3  978302109\n",
            "2  1   914  3  978301968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training and test sets\n",
        "\n",
        "# Paths\n",
        "training_set_file = os.path.join(drive_path, 'ml-100k/u1.base')\n",
        "test_set_file = os.path.join(drive_path, 'ml-100k/u1.test')\n",
        "\n",
        "# Load\n",
        "# 0th is row, 1st column = user, 2nd = movie, 3rd = rating, 4th = timestamp\n",
        "training_set = pd.read_csv(training_set_file, delimiter = '\\t')\n",
        "test_set = pd.read_csv(test_set_file, delimiter = '\\t')\n",
        "\n",
        "# Display\n",
        "print(' TRAINING', training_set.head(3), '\\n\\n', 'TESTING', test_set.head(3))\n",
        "\n",
        "# Turn to arrays (same values still)\n",
        "training_set = np.array(training_set, dtype = 'int')\n",
        "test_set = np.array(test_set, dtype = 'int')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZBgMH7HBkeq",
        "outputId": "b5746ebe-694a-4f4a-c390-8383dab439d9"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " TRAINING    1  1.1  5  874965758\n",
            "0  1    2  3  876893171\n",
            "1  1    3  4  878542960\n",
            "2  1    4  3  876893119 \n",
            "\n",
            " TESTING    1   6  5  887431973\n",
            "0  1  10  3  875693118\n",
            "1  1  12  5  878542960\n",
            "2  1  14  5  874965706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get total number of users and movies, across\n",
        "# Gives total across train and test data (cross-validation)\n",
        "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
        "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"
      ],
      "metadata": {
        "id": "ePkmK0J1HWFw"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data into array with a user on each line and movies in columns\n",
        "def convert(data):\n",
        "    # Create list of list. Each list corresponds to a user, and their movie ratings\n",
        "    new_data = []\n",
        "    # Add ratings into user's list\n",
        "    for id_users in range(1, nb_users + 1):\n",
        "      id_movies = data[:, 1][data[:, 0] == id_users]\n",
        "      id_ratings = data[:, 2][data[:, 0] == id_users]\n",
        "      # Fill with zeros\n",
        "      ratings = np.zeros(nb_movies)\n",
        "      # Replace zeros with real ratings\n",
        "      ratings[id_movies - 1] = id_ratings\n",
        "      new_data.append(list(ratings))\n",
        "    return new_data\n",
        "\n",
        "# Contains 943 rows of lists. In each list is the user's ratings of each movie\n",
        "# Moves without a rating just have a 0\n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)"
      ],
      "metadata": {
        "id": "7yyCKXLYJW9P"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data into Torch tensors to enable manipulation in PyTorch\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)"
      ],
      "metadata": {
        "id": "d5-oVnIkM1H_"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up to here, all the data preprocessing could be used for other types of models. After here, it's specific to Botlzmann models"
      ],
      "metadata": {
        "id": "O5SrDBvdN6zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert ratings into binary (1 = liked, 0 = not liked, -1 = no rating)\n",
        "training_set[training_set == 0] = -1\n",
        "training_set[training_set == 1] = 0\n",
        "training_set[training_set == 2] = 0\n",
        "training_set[training_set >= 3] = 1\n",
        "\n",
        "test_set[test_set == 0] = -1\n",
        "test_set[test_set == 1] = 0\n",
        "test_set[test_set == 2] = 0\n",
        "test_set[test_set >= 3] = 1"
      ],
      "metadata": {
        "id": "m9adBwqGOGtH"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boltzmann model"
      ],
      "metadata": {
        "id": "QuTwCNGCPwSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RBM():\n",
        "  # Self reference, visible nodes, hidden nodes\n",
        "  def __init__(self, nv, nh):\n",
        "    # Initialise weights\n",
        "    self.W = torch.randn(nh, nv)\n",
        "    # Probability of hidden nodes, given visible nodes. 1 = batch, nh = bias\n",
        "    self.a = torch.randn(1, nh)\n",
        "    # Bias for visible nodes\n",
        "    self.b = torch.randn(1, nv)\n",
        "  # Calculate probability that hidden neuron h = 1 given the visible neuron\n",
        "  def sample_h(self, x):\n",
        "    # Make product of two tensors. 'mm' does this for two torch tensors. x = visible neuron, w = tensor of weights\n",
        "    wx = torch.mm(x, self.W.t())\n",
        "    # Activation function. wx + bias. Apply bias to each line of mini-batch using expand_as\n",
        "    activation = wx + self.a.expand_as(wx)\n",
        "    # Probability that hidden node is activated, given visible node. Calculated as sigmoid of activation\n",
        "    p_h_given_v = torch.sigmoid(activation)\n",
        "    # If random number is below 70%, activate neuron, otherwise don't. Gives 0 & 1\n",
        "    return p_h_given_v, torch.bernoulli(p_h_given_v)\n",
        "  # Calculate probability that visible neuron v = 1 given the hidden neuron\n",
        "  def sample_v(self, y):\n",
        "    # Transpose not needed. W is weight matrix of pv given h so you need transpose for ph given v. Here it's just pv given h though so no transpose\n",
        "    wy = torch.mm(y, self.W)\n",
        "    # b, not a\n",
        "    activation = wy + self.b.expand_as(wy)\n",
        "    p_v_given_h = torch.sigmoid(activation)\n",
        "    # From vector of probabilities, give some sampling. If random number from sampling is below 0.25, give 1, otherwise 0\n",
        "    # Depending on 0 or 1, this is the prediction of whether or not user will give a like\n",
        "    return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
        "# v0 = input vector containing ratings of all the movies by one user\n",
        "# vk = visible nodes obtained after k-sampling\n",
        "# ph0 = vector of probabilities which at first iteration have hidden nodes h = 1, given the values of v0\n",
        "# phk = probability of hidden nodes after k-sampling given values of visible nodes vk\n",
        "# Other parameters like learning rate could be added to improve model\n",
        "  def train(self, v0, vk, ph0, phk):\n",
        "    self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n",
        "    self.b += torch.sum((v0 - vk), 0)\n",
        "    self.a += torch.sum((ph0 - phk), 0)\n",
        "# Nodes are ratings of all movies by a user in this example so NV (number of visible nodes) is the number of movies\n",
        "nv = len(training_set[0])\n",
        "# nh can be any number but there are 1,682 movies. nh corresponds to actual features so maybe 100 features. Best to tune and try different numbers\n",
        "nh = 100\n",
        "# Also tuneable\n",
        "batch_size = 100\n",
        "# Create RBM object\n",
        "rbm = RBM(nv, nh)"
      ],
      "metadata": {
        "id": "4YWB9U9rPyh2"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RBM is an energy-based model i.e., you're trying to minimize the energy function for a normal state. Energy depends on weighting of the model based on the tensors.\n",
        "\n",
        "Weights can be optimized to minimize the energy of the model. Minimizing the energy is equivalent to maximizing the log-likelihood gradient of the training set, so that's what you're computing in the model.\n",
        "\n",
        "Computing the log-likelihood gradient of the training set directly is too computationally expensive, so instead you can reach it through better and better approximations. Tiny adjustments in the direction of minimal energy.\n",
        "\n",
        "Contrastive divergence learning allows you to get those adjustments. You can do this using a Gibbs chain in k number of steps. K * hidden nodes and visible nodes. So given v0, sample the probable hidden nodes for v0 and then use those probable hidden nodes to sample the probable visible nodes. Repeat for v1, v2...vK. That's a CDK algorithm: k-step contrastive divergence."
      ],
      "metadata": {
        "id": "CY8l3_axe_CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training RBM model\n",
        "\n",
        "nb_epoch = 10\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "  # Need loss function to measure error between predictions and observations\n",
        "  # Could use RMSE (root mean squared error), but also simple distance or absolute distance\n",
        "  # Loss starts at 0 and increases with errors\n",
        "  train_loss = 0\n",
        "  # Also need a counter to normalize train_loss by dividing train_loss by counter\n",
        "  s = 0.\n",
        "  # Update weights in batches of users, not per each user. Batch size 100\n",
        "  for id_user in range(0, nb_users - batch_size, 100):\n",
        "    # Input is vector going into Gibbs chain. Ratings of all the users in the batch\n",
        "    vk = training_set[id_user : id_user + batch_size]\n",
        "    # Movie ratings from batch. Same at start, will be updated\n",
        "    v0 = training_set[id_user : id_user + batch_size]\n",
        "    # Need to get initial probabilities. Ph0 = probabilities that h nodes = 1 at start, given user ratings\n",
        "    ph0,_ = rbm.sample_h(v0)\n",
        "    # For loop takes you through samples of hidden and visible nodes. Goes until 10th sample\n",
        "    for k in range(10):\n",
        "      # Hidden nodes obtained at k-th step of contrastive divergence\n",
        "      _,hk = rbm.sample_h(vk)\n",
        "      # First update of visible nodes after random sampling\n",
        "      _,vk = rbm.sample_v(hk)\n",
        "      # Some cells contain no ratings (coded as -1). To prevent -1 ratings from interfering with\n",
        "      # updating process, do this to keep -1 ratings:\n",
        "      vk[v0 < 0] = v0[v0 < 0]\n",
        "    # Apply train function to update weights & bias\n",
        "    phk,_ = rbm.sample_h(vk)\n",
        "    rbm.train(v0, vk, ph0, phk)\n",
        "    # Update train loss, using absolute difference between predictions and actual values\n",
        "    train_loss += torch.mean(torch.abs(v0[v0 >= 0] - vk[v0 >= 0]))\n",
        "    s += 1.\n",
        "  print('epoch: ' + str(epoch) + '  loss: ' + str(train_loss / s))"
      ],
      "metadata": {
        "id": "DzN87f1XmeqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6955b7d5-9cc6-49ec-f34c-e5c667d6c641"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1  loss: tensor(0.3273)\n",
            "epoch: 2  loss: tensor(0.2490)\n",
            "epoch: 3  loss: tensor(0.2497)\n",
            "epoch: 4  loss: tensor(0.2503)\n",
            "epoch: 5  loss: tensor(0.2489)\n",
            "epoch: 6  loss: tensor(0.2494)\n",
            "epoch: 7  loss: tensor(0.2472)\n",
            "epoch: 8  loss: tensor(0.2460)\n",
            "epoch: 9  loss: tensor(0.2485)\n",
            "epoch: 10  loss: tensor(0.2488)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Better attempt at explaining a Gibbs chain:\n",
        "\n",
        "If you've got a dataset with related variables, there should be some kind of 'typical' configuration of the variables which you could infer from the actual values. Like a normal distribution across multiple variables, taking into account how they interact with each other.\n",
        "\n",
        "When you're looking at your data for the first time, you don't know exactly what that idealized configuration of the variables would look like, but you can figure it out using a Gibbs chain.\n",
        "\n",
        "To do that, you could take a row of data from the table \\(let's say values v1, v2, and v3 from three variables). Blank out v3, and see what v1 and v2 suggest that v3 is likely to be. Then update v3, blank out v1, and use the new v3 and v2 to predict v1. Then use updated v3 and v2 to predict v1 and so on. You keep doing this round and round until you reach a resting state where the variables stop updating in any significant way.\n",
        "\n",
        "That state represents \\(roughly) the most likely state that you'll probably see the variables distributed in, given the relationships between them."
      ],
      "metadata": {
        "id": "bZfRb63xz-Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing RBM\n",
        "# Code essentially being reused from above minus a few small cuts and edits for testing, not training\n",
        "\n",
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "  # v is input on which you're making the prediction\n",
        "  v = training_set[id_user : id_user + 1]\n",
        "  # vt is target, not previous data\n",
        "  vt = test_set[id_user : id_user + 1]\n",
        "  if len(vt[vt >= 0]) > 0:\n",
        "    # No k steps\n",
        "    _,h = rbm.sample_h(v)\n",
        "    _,v = rbm.sample_v(h)\n",
        "    # Still take absolute distance between prediction and target\n",
        "    test_loss += torch.mean(torch.abs(vt[vt >= 0] - v[vt >= 0]))\n",
        "    s += 1.\n",
        "print('Test loss: ' + str(test_loss / s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uDJWN0-9zI8",
        "outputId": "3793f0dc-f7c6-4f05-acf1-7ed635b4e90c"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: tensor(0.2455)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When testing the RBM model, you don't need the for loop to run over 10 ephochs because you're only trying to predict a single value. Since you're only making one prediction instead of 10, in theory it should be an accurate prediction \\(relates to MCMC)\n",
        "\n",
        "Model can now predict binary ratings \\(0 or 1) pretty accurately"
      ],
      "metadata": {
        "id": "pCTZPhUoCJkT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}